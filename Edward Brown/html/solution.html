
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>solution</title><meta name="generator" content="MATLAB 9.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-05-30"><meta name="DC.source" content="solution.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1"><b>Support Vector Machine Classification on MNIST digit data</b></a></li><li><a href="#2">Primal Solution</a></li><li><a href="#3">Gradient Descent with Line Search</a></li><li><a href="#4">Gradient Descent with Backtracking</a></li><li><a href="#5">Conjugate Gradient Descent with Line Search</a></li><li><a href="#6">Stochastic Gradient Descent Hyperameter Optimisation</a></li><li><a href="#7">Stochastic Gradient Descent Momentum Analysis(SGD)</a></li><li><a href="#8">Dual Solution</a></li><li><a href="#9">SMO</a></li><li><a href="#10">Residual Redistribution Algorithm</a></li><li><a href="#11">Residual Redistribution Algorithm: Modification 1</a></li><li><a href="#12">Residual Redistribution Algorithm: Modification 2</a></li></ul></div><h2><b>Support Vector Machine Classification on MNIST digit data</b><a name="1"></a></h2><p>Edward Brown: 16100321 Please Note this script takes approximately 5 minutes to run Thanks to Marta Betcke for providing the backtracking, linesearch and conjugate gradient functions. As well as her help and teaching throughout the course.</p><pre class="codeinput">clear <span class="string">all</span>, close <span class="string">all</span>;


<span class="comment">% Initial Parameters</span>
rng(50);
lambda = 0.1;
nTrain = 100;
[X,y,X_test,y_test] = load_data(nTrain);
repeat_count = 10;
ws = randn(repeat_count,size(X,2));
alpha0 = 100;
c1 = 1e-4;
tol = 1e-3;
maxIter = 200;
alpha_max = alpha0;
</pre><h2>Primal Solution<a name="2"></a></h2><p>Function definitions for use with tutorial code</p><pre class="codeinput">F.f = @(w) cost(w, X, y, lambda);
F.df = @(w) primal_derivative(w, X, y, lambda);
</pre><h2>Gradient Descent with Line Search<a name="3"></a></h2><p>Gradient Descent is performed with line search obeying strong Wolfe Conditions</p><pre class="codeinput">lsOptsSteepLS.c1 = c1;
lsOptsSteepLS.c2 = 0.3;
lsFun = @(x_k, p_k, alpha0) lineSearch_new(F, <span class="keyword">...</span>
    x_k, p_k, alpha_max, lsOptsSteepLS);

<span class="keyword">for</span> i = 1:repeat_count
    [w, ~, nIterSteepLS(i), infoSteepLS] = <span class="keyword">...</span>
        descentLineSearch(F, <span class="string">'steepest'</span>, lsFun, alpha0, ws(i,:), tol, maxIter);
<span class="keyword">end</span>

meanSteepLS = mean(nIterSteepLS);
stdSteepLS = std(nIterSteepLS);
</pre><h2>Gradient Descent with Backtracking<a name="4"></a></h2><p>Gradient Descent is performed with backtracking Conditions</p><pre class="codeinput">lsOptsSteepBT.rho = 0.1;
lsOptsSteepBT.c1 = c1;
lsFun = @(x_k, p_k, alpha0) backtracking(F, <span class="keyword">...</span>
    x_k, p_k, alpha0, lsOptsSteepBT);

<span class="keyword">for</span> i = 1:repeat_count
    [~, ~, nIterSteepBT(i), infoSteepBT] = <span class="keyword">...</span>
        descentLineSearch(F, <span class="string">'steepest'</span>, lsFun, alpha0, ws(i,:), tol, maxIter);
<span class="keyword">end</span>

meanSteepBT = mean(nIterSteepBT);
stdSteepBT = std(nIterSteepBT);
</pre><h2>Conjugate Gradient Descent with Line Search<a name="5"></a></h2><p>Conjugate Gradient is performed with the beta values for Polak-Ribiere and Fletcher-Reeves Conditions</p><pre class="codeinput"><span class="comment">% Polak Ribiere</span>
lsOptsCGPR.c1 = c1;
lsOptsCGPR.c2 = 0.3;
ls = @(x_k, p_k, alpha0) lineSearch_new(F, <span class="keyword">...</span>
    x_k, p_k, alpha0, lsOptsCGPR);

<span class="keyword">for</span> i = 1:repeat_count
    [~, ~, nIterCGPR(i), infoCGPR] = <span class="keyword">...</span>
        nonlinearConjugateGradient(F, ls, <span class="string">'PR'</span>, alpha0, ws(i,:),<span class="keyword">...</span>
        tol, maxIter);
<span class="keyword">end</span>

meanCGPR = mean(nIterCGPR);
stdCGPR = std(nIterCGPR);

<span class="comment">% Fletcher-Reeves</span>
lsOptsCGFR.c1 = c1;
lsOptsCGFR.c2 = 0.1;
ls = @(x_k, p_k, alpha0) lineSearch_new(F, <span class="keyword">...</span>
    x_k, p_k, 100, lsOptsCGFR);

<span class="keyword">for</span> i = 1:repeat_count
    w0 = randn(1,size(X,2));
    [~, ~, nIterCGFR(i), infoCGFR] = <span class="keyword">...</span>
        nonlinearConjugateGradient(F, ls, <span class="string">'FR'</span>, alpha0, ws(i,:),<span class="keyword">...</span>
        tol, maxIter);
<span class="keyword">end</span>

meanCGFR = mean(nIterCGFR);
stdCGFR = std(nIterCGFR);

<span class="comment">% Plot Analyses</span>
plot(log(infoSteepLS.cost)); hold <span class="string">on</span>;
plot(log(infoSteepBT.cost));hold <span class="string">on</span>;
plot(log(infoCGPR.cost)); hold <span class="string">on</span>;
plot(log(infoCGFR.cost));hold <span class="string">on</span>;

<span class="comment">% Format Plot</span>
title(<span class="string">'Log Costs for random initial weighting'</span>)
legend(<span class="string">'Gradient Descent BackTracking'</span>,<span class="keyword">...</span>
    <span class="string">'Gradient Descent Line Search'</span>,<span class="keyword">...</span>
    <span class="string">'CG Polak-Ribiere'</span>,<span class="keyword">...</span>
    <span class="string">'CG Fletcher-Reeves'</span>)
xlabel(<span class="string">'Iteration'</span>)
ylabel(<span class="string">'Log Cost'</span>)

figure
c = {<span class="string">'Gradient Descent Back Tracking'</span>,<span class="keyword">...</span>
    <span class="string">'Gradient Descent Line Search'</span>,<span class="keyword">...</span>
    <span class="string">'CG Polak-Ribiere'</span>,<span class="keyword">...</span>
    <span class="string">'CG Fletcher-Reeves'</span>
};

means = [meanSteepLS,meanSteepBT,meanCGPR,meanCGFR];
bar(means)
set(gca, <span class="string">'XTickLabel'</span>,c, <span class="string">'XTick'</span>,1:numel(c))
ax = gca;
ax.XTickLabelRotation = -15;
ylabel(<span class="string">'Mean Iteration Count'</span>);
title(<span class="string">'Mean Iteration Count until Convergence'</span>)

<span class="comment">% As the above graph shows, Polak-Ribiere was a much more effective that</span>
<span class="comment">% Fletcher-Reeves. The simple gradient descent was comparably effective</span>
<span class="comment">% with both backtracking and line search. The chart does not however enable</span>
<span class="comment">% us to say which is preferable because the standard deviations were very</span>
<span class="comment">% high, and each of the best 3 of four were all less than one deviation</span>
<span class="comment">% from eachother.</span>
</pre><img vspace="5" hspace="5" src="solution_01.png" alt=""> <img vspace="5" hspace="5" src="solution_02.png" alt=""> <h2>Stochastic Gradient Descent Hyperameter Optimisation<a name="6"></a></h2><p>This part was moved out to the function 'stochastic_hyper_parameter' due to its size</p><pre class="codeinput">stochastic_hyper_parameter
</pre><img vspace="5" hspace="5" src="solution_03.png" alt=""> <img vspace="5" hspace="5" src="solution_04.png" alt=""> <img vspace="5" hspace="5" src="solution_05.png" alt=""> <img vspace="5" hspace="5" src="solution_06.png" alt=""> <img vspace="5" hspace="5" src="solution_07.png" alt=""> <img vspace="5" hspace="5" src="solution_08.png" alt=""> <img vspace="5" hspace="5" src="solution_09.png" alt=""> <img vspace="5" hspace="5" src="solution_10.png" alt=""> <h2>Stochastic Gradient Descent Momentum Analysis(SGD)<a name="7"></a></h2><p>This section compares the results of SGD for simple gradient descent, gradient descent with momentum and one with a pre-evaluated gamma schedule.</p><pre class="codeinput">[X,y,X_test,y_test] = load_data(1000);

<span class="keyword">for</span> i = 1:10
    <span class="comment">% Initialise Weight</span>
    w0 = ws(i,:);

    <span class="comment">% Assign SGD parameters</span>
    sgdOpts.batch_size = 1;
    sgdOpts.learning_rate = 0.02;
    sgdOpts.tol = 10^-6;
    sgdOpts.lambda = lambda;
    sgdOpts.max_iter = 15;
    sgdOpts.gamma = 0.4;
    sgdOpts.gamma_init = 0.4;
    sgdOpts.gamma_final = 0.9;

    <span class="comment">% Performs SGD for the three regimes</span>
    sgdOpts.method = <span class="string">'normal'</span>;
    sgdOpts.learning_rate = 0.01;
    infoSGD = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);

    sgdOpts.method = <span class="string">'momentum'</span>;
    sgdOpts.learning_rate = 0.01;
    infoSGDMom = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);

    sgdOpts.method = <span class="string">'plan'</span>;
    sgdOpts.learning_rate = 0.01;
    infoSGDPlan = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);

    <span class="comment">% Plot cost graph for trial iteration</span>
    <span class="keyword">if</span> i == 1
        figure;
        plot(log(infoSGD.costs)); hold <span class="string">on</span>;
        plot(log(infoSGDMom.costs)); hold <span class="string">on</span>;
        plot(log(infoSGDPlan.costs)); hold <span class="string">on</span>;
        xlabel(<span class="string">'Iteration'</span>);
        ylabel(<span class="string">'Cost'</span>)
        title(<span class="string">'Cost by Iteration'</span>)
        legend(<span class="string">'SGD'</span>, <span class="string">'SGD w Momentum'</span>, <span class="string">'SGD w Momentum schedule'</span>);
    <span class="keyword">end</span>
    sgdIter(i) = infoSGD.nIter;
    sgdIterMom(i) = infoSGDMom.nIter;
    sgdIterPlan(i) = infoSGDPlan.nIter;
<span class="keyword">end</span>

figure
plot(infoSGD.test_accs); hold <span class="string">on</span>;
plot(infoSGDMom.test_accs); hold <span class="string">on</span>;
plot(infoSGDPlan.test_accs); hold <span class="string">on</span>;
xlabel(<span class="string">'Iterations'</span>)
ylabel(<span class="string">'Test Accuracy'</span>)
legend(<span class="string">'SGD'</span>, <span class="string">'SGD w Momentum'</span>, <span class="string">'SGD w Momentum schedule'</span>, <span class="keyword">...</span>
    <span class="string">'Location'</span>, <span class="string">'southeast'</span>);
title(<span class="string">'Test Accuracy by Iteration'</span>)
</pre><img vspace="5" hspace="5" src="solution_11.png" alt=""> <img vspace="5" hspace="5" src="solution_12.png" alt=""> <h2>Dual Solution<a name="8"></a></h2><p>For this part we will employ a simplified version of the SMO algorithm (Platt 1999) and a descent algorithm of, to the best of my knowledge, my own making. Or at least, I cannot seem to find it elsewhere. There is a more than likely chance that there are algorithms which are the same as this one, or it is a very dumb version of other more sophisticated versions.</p><h2>SMO<a name="9"></a></h2><p>Here is a simplified form of the SMO. Found here: The Simplified SMO Algorithm. Autumn 2009. <a href="http://cs229.stanford.edu/materials/smo.pdf">http://cs229.stanford.edu/materials/smo.pdf</a></p><pre class="codeinput"><span class="comment">% The algorithm was written with help from this resource:</span>
<span class="comment">% https://uk.mathworks.com/matlabcentral/</span>
<span class="comment">% fileexchange/63100-smo--sequential-minimal-optimization-</span>

<span class="comment">% Due to the algorithms size, it is run within this function</span>
<span class="comment">% 'sequential_minimisation'</span>
sequential_minimisation

<span class="comment">% As you can see the algorithm originally arrives at decent accuracies</span>
<span class="comment">% quite quickly.</span>
</pre><pre class="codeoutput">
train_accuracy =

    0.9450


test_accuracy =

    0.9160

</pre><img vspace="5" hspace="5" src="solution_13.png" alt=""> <h2>Residual Redistribution Algorithm<a name="10"></a></h2><p>Here is the fun part. Load Preprocessed Data</p><pre class="codeinput">rng(42)
[X,y,X_test,y_test] = load_data(1000);

<span class="comment">% Parameters</span>
<span class="comment">% rng(42);</span>
C = 0.02;
[data_size, dimensions] = size(X);
<span class="comment">% lambda = 2/(data_size * C);</span>
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

<span class="comment">% Initialise weight/alphas vector</span>
alphas = half_c * ones(size(y));

<span class="comment">% Start loop</span>
<span class="comment">% Perform iteration for number of epochs</span>
<span class="keyword">for</span> epoch_index = 1:epochs
    <span class="keyword">for</span> i = 1:data_size
        old_alpha = alphas(i);

        <span class="comment">% Update alpha using gradient of Lagrangian</span>
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        <span class="comment">% Clip Alpha to [0,C]</span>
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        <span class="comment">% Assign updated alpha value</span>
        alphas(i) = new_alpha;
        <span class="keyword">if</span> isnan(alpha_update)
            disp(<span class="string">'ERROR'</span>)
        <span class="keyword">end</span>

        <span class="comment">% Redistribute the residual. (A lot of hype for two lines of code)</span>
        residual = y' * alphas;
        alphas = alphas  - 1/sum(alphas) * (residual) * y .* alphas;
    <span class="keyword">end</span>

    <span class="comment">% Compute Accuracy after each pass</span>
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = <span class="keyword">...</span>
        mean(sign(prediction(w,X_test)+ bias) == y_test);
<span class="keyword">end</span>

<span class="comment">% Output analysis</span>
figure;plot(test_acc)
hold <span class="string">on</span>;plot(train_acc)
legend(<span class="string">'Test'</span>, <span class="string">'Train'</span>)
title(<span class="string">'Accuracy per pass'</span>)
xlabel(<span class="string">'Iteration'</span>)
ylabel(<span class="string">'Accuracy'</span>)
</pre><img vspace="5" hspace="5" src="solution_14.png" alt=""> <h2>Residual Redistribution Algorithm: Modification 1<a name="11"></a></h2><p>Redistribute less often  Load Preprocessed Data</p><pre class="codeinput">rng(42)
[X,y,X_test,y_test] = load_data(1000);

<span class="comment">% Parameters</span>
<span class="comment">% rng(42);</span>
C = 0.02;
[data_size, dimensions] = size(X);
<span class="comment">% lambda = 2/(data_size * C);</span>
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

<span class="comment">% Initialise weight/alphas vector</span>
alphas = half_c * ones(size(y));

<span class="comment">% Start loop</span>
<span class="comment">% Perform iteration for number of epochs</span>
<span class="keyword">for</span> epoch_index = 1:epochs
    <span class="keyword">for</span> i = 1:data_size
        old_alpha = alphas(i);

        <span class="comment">% Update alpha using gradient of Lagrangian</span>
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        <span class="comment">% Clip Alpha to [0,C]</span>
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        <span class="comment">% Assign updated alpha value</span>
        alphas(i) = new_alpha;
        <span class="keyword">if</span> isnan(alpha_update)
            disp(<span class="string">'ERROR'</span>)
        <span class="keyword">end</span>

        <span class="comment">% Redistribute the residual less often!</span>
        <span class="keyword">if</span> rem(i,500) == 0
            residual = y' * alphas;
            alphas = alphas  - 1/sum(alphas) * (residual) * y .* alphas;
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% Compute Accuracy after each pass</span>
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = <span class="keyword">...</span>
        mean(sign(prediction(w,X_test)+ bias) == y_test);
<span class="keyword">end</span>

<span class="comment">% Output analysis</span>
figure;plot(test_acc)
hold <span class="string">on</span>;plot(train_acc)
legend(<span class="string">'Test'</span>, <span class="string">'Train'</span>)
title(<span class="string">'Accuracy per pass for modification 1'</span>)
xlabel(<span class="string">'Iteration'</span>)
ylabel(<span class="string">'Accuracy'</span>)
</pre><img vspace="5" hspace="5" src="solution_15.png" alt=""> <h2>Residual Redistribution Algorithm: Modification 2<a name="12"></a></h2><p>Don't Redistribute  Load Preprocessed Data</p><pre class="codeinput">rng(42)
[X,y,X_test,y_test] = load_data(1000);

<span class="comment">% Parameters</span>
<span class="comment">% rng(42);</span>
C = 0.02;
[data_size, dimensions] = size(X);
<span class="comment">% lambda = 2/(data_size * C);</span>
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

<span class="comment">% Initialise weight/alphas vector</span>
alphas = half_c * ones(size(y));

<span class="comment">% Start loop</span>
<span class="comment">% Perform iteration for number of epochs</span>
<span class="keyword">for</span> epoch_index = 1:epochs
    <span class="keyword">for</span> i = 1:data_size
        old_alpha = alphas(i);

        <span class="comment">% Update alpha using gradient of Lagrangian</span>
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        <span class="comment">% Clip Alpha to [0,C]</span>
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        <span class="comment">% Assign updated alpha value</span>
        alphas(i) = new_alpha;
        <span class="keyword">if</span> isnan(alpha_update)
            disp(<span class="string">'ERROR'</span>)
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% Compute Accuracy after each pass</span>
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = <span class="keyword">...</span>
        mean(sign(prediction(w,X_test)+ bias) == y_test);
<span class="keyword">end</span>

<span class="comment">% Output analysis</span>
figure;plot(test_acc)
hold <span class="string">on</span>;plot(train_acc)
legend(<span class="string">'Test'</span>, <span class="string">'Train'</span>)
title(<span class="string">'Accuracy per pass: Modification 2'</span>)
xlabel(<span class="string">'Iteration'</span>)
ylabel(<span class="string">'Accuracy'</span>)
</pre><img vspace="5" hspace="5" src="solution_16.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% *Support Vector Machine Classification on MNIST digit data*
% Edward Brown: 16100321
% Please Note this script takes approximately 5 minutes to run
% Thanks to Marta Betcke for providing the backtracking, linesearch and
% conjugate gradient functions. As well as her help and teaching
% throughout the course.

clear all, close all;


% Initial Parameters
rng(50);
lambda = 0.1;
nTrain = 100;
[X,y,X_test,y_test] = load_data(nTrain);
repeat_count = 10;
ws = randn(repeat_count,size(X,2));
alpha0 = 100;
c1 = 1e-4; 
tol = 1e-3;
maxIter = 200;
alpha_max = alpha0;

%% Primal Solution
% Function definitions for use with tutorial code
F.f = @(w) cost(w, X, y, lambda);
F.df = @(w) primal_derivative(w, X, y, lambda);

%% Gradient Descent with Line Search
% Gradient Descent is performed with line search obeying strong Wolfe
% Conditions
lsOptsSteepLS.c1 = c1;
lsOptsSteepLS.c2 = 0.3;
lsFun = @(x_k, p_k, alpha0) lineSearch_new(F, ...
    x_k, p_k, alpha_max, lsOptsSteepLS);

for i = 1:repeat_count
    [w, ~, nIterSteepLS(i), infoSteepLS] = ...
        descentLineSearch(F, 'steepest', lsFun, alpha0, ws(i,:), tol, maxIter);
end

meanSteepLS = mean(nIterSteepLS);
stdSteepLS = std(nIterSteepLS);

%% Gradient Descent with Backtracking
% Gradient Descent is performed with backtracking
% Conditions
lsOptsSteepBT.rho = 0.1;
lsOptsSteepBT.c1 = c1;
lsFun = @(x_k, p_k, alpha0) backtracking(F, ...
    x_k, p_k, alpha0, lsOptsSteepBT);

for i = 1:repeat_count
    [~, ~, nIterSteepBT(i), infoSteepBT] = ...
        descentLineSearch(F, 'steepest', lsFun, alpha0, ws(i,:), tol, maxIter);
end

meanSteepBT = mean(nIterSteepBT);
stdSteepBT = std(nIterSteepBT);

%% Conjugate Gradient Descent with Line Search
% Conjugate Gradient is performed with the beta values for Polak-Ribiere
% and Fletcher-Reeves
% Conditions

% Polak Ribiere
lsOptsCGPR.c1 = c1;
lsOptsCGPR.c2 = 0.3;
ls = @(x_k, p_k, alpha0) lineSearch_new(F, ...
    x_k, p_k, alpha0, lsOptsCGPR);

for i = 1:repeat_count
    [~, ~, nIterCGPR(i), infoCGPR] = ...
        nonlinearConjugateGradient(F, ls, 'PR', alpha0, ws(i,:),...
        tol, maxIter);
end

meanCGPR = mean(nIterCGPR);
stdCGPR = std(nIterCGPR);

% Fletcher-Reeves
lsOptsCGFR.c1 = c1;
lsOptsCGFR.c2 = 0.1;
ls = @(x_k, p_k, alpha0) lineSearch_new(F, ...
    x_k, p_k, 100, lsOptsCGFR);

for i = 1:repeat_count
    w0 = randn(1,size(X,2));
    [~, ~, nIterCGFR(i), infoCGFR] = ...
        nonlinearConjugateGradient(F, ls, 'FR', alpha0, ws(i,:),...
        tol, maxIter);
end

meanCGFR = mean(nIterCGFR);
stdCGFR = std(nIterCGFR);

% Plot Analyses
plot(log(infoSteepLS.cost)); hold on;
plot(log(infoSteepBT.cost));hold on;
plot(log(infoCGPR.cost)); hold on;
plot(log(infoCGFR.cost));hold on;

% Format Plot
title('Log Costs for random initial weighting')
legend('Gradient Descent BackTracking',...
    'Gradient Descent Line Search',...
    'CG Polak-Ribiere',...
    'CG Fletcher-Reeves')
xlabel('Iteration')
ylabel('Log Cost')

figure
c = {'Gradient Descent Back Tracking',...
    'Gradient Descent Line Search',...
    'CG Polak-Ribiere',...
    'CG Fletcher-Reeves'
};

means = [meanSteepLS,meanSteepBT,meanCGPR,meanCGFR];
bar(means)
set(gca, 'XTickLabel',c, 'XTick',1:numel(c))
ax = gca;
ax.XTickLabelRotation = -15;
ylabel('Mean Iteration Count');
title('Mean Iteration Count until Convergence')

% As the above graph shows, Polak-Ribiere was a much more effective that
% Fletcher-Reeves. The simple gradient descent was comparably effective
% with both backtracking and line search. The chart does not however enable
% us to say which is preferable because the standard deviations were very
% high, and each of the best 3 of four were all less than one deviation
% from eachother.

%% Stochastic Gradient Descent Hyperameter Optimisation
% This part was moved out to the function 'stochastic_hyper_parameter' due to its size
stochastic_hyper_parameter

%% Stochastic Gradient Descent Momentum Analysis(SGD)
% This section compares the results of SGD for simple gradient descent,
% gradient descent with momentum and one with a pre-evaluated gamma
% schedule. 

[X,y,X_test,y_test] = load_data(1000);

for i = 1:10
    % Initialise Weight
    w0 = ws(i,:);
 
    % Assign SGD parameters
    sgdOpts.batch_size = 1;
    sgdOpts.learning_rate = 0.02;
    sgdOpts.tol = 10^-6;
    sgdOpts.lambda = lambda;
    sgdOpts.max_iter = 15;
    sgdOpts.gamma = 0.4;
    sgdOpts.gamma_init = 0.4;
    sgdOpts.gamma_final = 0.9;
    
    % Performs SGD for the three regimes
    sgdOpts.method = 'normal';
    sgdOpts.learning_rate = 0.01;
    infoSGD = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);
    
    sgdOpts.method = 'momentum';
    sgdOpts.learning_rate = 0.01;
    infoSGDMom = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);
    
    sgdOpts.method = 'plan';
    sgdOpts.learning_rate = 0.01;
    infoSGDPlan = sgd_momentum_plan(X,y,X_test,y_test,w0,sgdOpts);

    % Plot cost graph for trial iteration
    if i == 1
        figure;
        plot(log(infoSGD.costs)); hold on;
        plot(log(infoSGDMom.costs)); hold on;
        plot(log(infoSGDPlan.costs)); hold on;
        xlabel('Iteration');
        ylabel('Cost')
        title('Cost by Iteration')
        legend('SGD', 'SGD w Momentum', 'SGD w Momentum schedule');
    end
    sgdIter(i) = infoSGD.nIter;
    sgdIterMom(i) = infoSGDMom.nIter;
    sgdIterPlan(i) = infoSGDPlan.nIter;
end

figure
plot(infoSGD.test_accs); hold on;
plot(infoSGDMom.test_accs); hold on;
plot(infoSGDPlan.test_accs); hold on;
xlabel('Iterations')
ylabel('Test Accuracy')
legend('SGD', 'SGD w Momentum', 'SGD w Momentum schedule', ...
    'Location', 'southeast');
title('Test Accuracy by Iteration')

%% Dual Solution
% For this part we will employ a simplified version of the
% SMO algorithm (Platt 1999) and
% a descent algorithm of, to the best of my knowledge, my own
% making. Or at least, I cannot seem to find it elsewhere. There is a
% more than likely chance that there are algorithms which are the same
% as this one, or it is a very dumb version of other more sophisticated
% versions.
% 
%% SMO
% Here is a simplified form of the SMO. Found here:
% The Simplified SMO Algorithm. Autumn 2009.
% http://cs229.stanford.edu/materials/smo.pdf

% The algorithm was written with help from this resource:
% https://uk.mathworks.com/matlabcentral/
% fileexchange/63100-smoREPLACE_WITH_DASH_DASHsequential-minimal-optimization-

% Due to the algorithms size, it is run within this function
% 'sequential_minimisation'
sequential_minimisation

% As you can see the algorithm originally arrives at decent accuracies
% quite quickly.

%% Residual Redistribution Algorithm
% Here is the fun part.
% Load Preprocessed Data
rng(42)
[X,y,X_test,y_test] = load_data(1000);

% Parameters
% rng(42);
C = 0.02;
[data_size, dimensions] = size(X);
% lambda = 2/(data_size * C);
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

% Initialise weight/alphas vector
alphas = half_c * ones(size(y));

% Start loop
% Perform iteration for number of epochs     
for epoch_index = 1:epochs
    for i = 1:data_size
        old_alpha = alphas(i);

        % Update alpha using gradient of Lagrangian
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        % Clip Alpha to [0,C]
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        % Assign updated alpha value
        alphas(i) = new_alpha;
        if isnan(alpha_update)
            disp('ERROR')
        end

        % Redistribute the residual. (A lot of hype for two lines of code)
        residual = y' * alphas;
        alphas = alphas  - 1/sum(alphas) * (residual) * y .* alphas;
    end

    % Compute Accuracy after each pass
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = ...
        mean(sign(prediction(w,X_test)+ bias) == y_test);
end

% Output analysis
figure;plot(test_acc)
hold on;plot(train_acc)
legend('Test', 'Train')
title('Accuracy per pass')
xlabel('Iteration')
ylabel('Accuracy')

%% Residual Redistribution Algorithm: Modification 1
% ** Redistribute less often **
% Load Preprocessed Data
rng(42)
[X,y,X_test,y_test] = load_data(1000);

% Parameters
% rng(42);
C = 0.02;
[data_size, dimensions] = size(X);
% lambda = 2/(data_size * C);
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

% Initialise weight/alphas vector
alphas = half_c * ones(size(y));

% Start loop
% Perform iteration for number of epochs     
for epoch_index = 1:epochs
    for i = 1:data_size
        old_alpha = alphas(i);

        % Update alpha using gradient of Lagrangian
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        % Clip Alpha to [0,C]
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        % Assign updated alpha value
        alphas(i) = new_alpha;
        if isnan(alpha_update)
            disp('ERROR')
        end

        % Redistribute the residual less often!
        if rem(i,500) == 0
            residual = y' * alphas;
            alphas = alphas  - 1/sum(alphas) * (residual) * y .* alphas;
        end
    end

    % Compute Accuracy after each pass
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = ...
        mean(sign(prediction(w,X_test)+ bias) == y_test);
end

% Output analysis
figure;plot(test_acc)
hold on;plot(train_acc)
legend('Test', 'Train')
title('Accuracy per pass for modification 1')
xlabel('Iteration')
ylabel('Accuracy')

%% Residual Redistribution Algorithm: Modification 2
% ** Don't Redistribute **
% Load Preprocessed Data
rng(42)
[X,y,X_test,y_test] = load_data(1000);

% Parameters
% rng(42);
C = 0.02;
[data_size, dimensions] = size(X);
% lambda = 2/(data_size * C);
half_c = C/2;
learning_rate = 0.0001;
epochs = 60;
ys = repmat(y,1,dimensions);
K = (ys.*X)*(ys .* X)';

% Initialise weight/alphas vector
alphas = half_c * ones(size(y));

% Start loop
% Perform iteration for number of epochs     
for epoch_index = 1:epochs
    for i = 1:data_size
        old_alpha = alphas(i);

        % Update alpha using gradient of Lagrangian
        alpha_update = learning_rate * (1 - update_alpha(K(i,:),alphas,i));

        % Clip Alpha to [0,C]
        new_alpha = old_alpha + alpha_update;
        new_alpha = min(C, new_alpha);
        new_alpha = max(0, new_alpha);

        % Assign updated alpha value
        alphas(i) = new_alpha;
        if isnan(alpha_update)
            disp('ERROR')
        end
    end

    % Compute Accuracy after each pass
    w = compute_weight(alphas, y, X);
    bias = mean(y - (X * w'));
    train_acc(epoch_index) = mean(sign(prediction(w,X) + bias) == y);
    test_acc(epoch_index) = ...
        mean(sign(prediction(w,X_test)+ bias) == y_test);
end

% Output analysis
figure;plot(test_acc)
hold on;plot(train_acc)
legend('Test', 'Train')
title('Accuracy per pass: Modification 2')
xlabel('Iteration')
ylabel('Accuracy')



##### SOURCE END #####
--></body></html>